import chromadb
from sentence_transformers import SentenceTransformer
import os
from crawler import scrape_article  # Assure-toi que crawler.py est correctement configurÃ©

# ğŸ—‚ï¸ Initialisation de ChromaDB (stockage persistant)
chroma_client = chromadb.PersistentClient(path="./chroma_db")

# ğŸ“Œ CrÃ©ation ou rÃ©cupÃ©ration de la collection
collection = chroma_client.get_or_create_collection(name="articles")

# ğŸ”¹ Chargement du modÃ¨le d'embeddings avec sentence-transformers
model = SentenceTransformer("all-MiniLM-L6-v2")  # Tu peux changer de modÃ¨le si besoin

def embed_text(text):
    """
    GÃ©nÃ¨re un embedding pour le texte Ã  l'aide de sentence-transformers.
    Affiche un extrait du vecteur gÃ©nÃ©rÃ© pour vÃ©rification.
    """
    embedding = model.encode(text).tolist()  # Conversion en liste pour compatibilitÃ© avec ChromaDB
    # Affiche les 10 premiÃ¨res valeurs du vecteur pour vÃ©rifier (tu peux afficher plus si nÃ©cessaire)
    print("Embedding gÃ©nÃ©rÃ© (extrait) :", embedding[:10])
    return embedding

def add_article(url):
    """
    RÃ©cupÃ¨re l'article via le scraper, gÃ©nÃ¨re son embedding et lâ€™ajoute Ã  ChromaDB.
    Affiche Ã©galement le vecteur complet pour vÃ©rification.
    """
    text = scrape_article(url)  # Scraping en direct depuis l'URL
    if not text:
        print(f"âŒ Impossible de rÃ©cupÃ©rer l'article {url}")
        return
    
    embedding = embed_text(text)
    # Pour voir le vecteur complet, dÃ©commente la ligne suivante (attention, cela peut Ãªtre long)
    print("Vecteur complet :", embedding)
    
    collection.add(
        ids=[url],                # Utilisation de l'URL comme identifiant unique
        embeddings=[embedding],   # Embedding gÃ©nÃ©rÃ© pour l'article
        metadatas=[{"url": url, "content": text}]  # Stockage des mÃ©tadonnÃ©es
    )
    print(f"âœ… Article {url} ajoutÃ© avec succÃ¨s !")

def search_similar(query, n_results=5):
    """
    Recherche les articles les plus proches du texte de la requÃªte.
    Affiche l'embedding de la requÃªte pour vÃ©rification.
    """
    query_embedding = embed_text(query)
    results = collection.query(query_embeddings=[query_embedding], n_results=n_results)
    
    if results["ids"]:
        return [(res, metadata["url"]) for res, metadata in zip(results["ids"][0], results["metadatas"][0])]
    else:
        return "âŒ Aucun article pertinent trouvÃ©."

# ğŸ› ï¸ TEST : Ajout d'un article rÃ©el via le scraper et recherche similaire
if __name__ == "__main__":
    # Remplace par une URL d'article rÃ©el que tu souhaites indexer
    url = "https://www.lemonde.fr/gouvernement-bayrou/"
    add_article(url)  # Scraping + Indexation
    
    # Effectuer une recherche avec une requÃªte
    query = "le gouvernement"
    results = search_similar(query)
    print("ğŸ“Œ RÃ©sultats de recherche :", results)
